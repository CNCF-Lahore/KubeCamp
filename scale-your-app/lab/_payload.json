[{"data":1,"prerenderedAt":212},["Reactive",2],{"search-api":3},[4,11,19,28,38,46,57,67,77,83,92,99,111,119,130,139,154,161,167,175,184,203],{"id":5,"path":6,"dir":7,"title":8,"description":7,"keywords":9,"body":10},"content:0.index.md","/","","Home",[],"    This Bootcamp Includes free Access to Kubernetes Playground, Lab Resources and Digital Badge.   We are still in the development stage, working on providing you a smooth experience with the Kubernetes Bootcamp, which includes updated content, a playground, and labs. Please feel free to contribute. Also, please complete 1 lab and quiz to receive the shareable badge. The badge can only be shared via email; for this, you need to complete the quiz.    The Bootcamp for Gaining Expertise in Kubernetes   The Kubernetes Mastery Bootcamp is a comprehensive, free training program that offers in-depth knowledge and hands-on experience in deploying, managing, and scaling containerized applications using Kubernetes. Participants will benefit from workshops, access to a sandbox environment for practical exercises, and training from industry experts.     Weekly Kubernetes Sessions from Kubernetes Experts   RSVP Here   5  Hands-on Explanation-based Kubernetes Labs  Dedicated Open Source   Kubernetes Playground  Dedicated Support From Kubernetes Experts on   Slack  Access Pass to Kubernetes Community Days 2024  Awesome Digital Badges for Skills and Achievements  Get a chance to avail LinkedIn Premium, swags, and a Kubernetes voucher by completing all the requirements.   Terms & Conditions Applied         To access the benefits, please complete all requirements:    Join Slack   Join the CNCF Slack and stay updated with Kubernetes Trainings and Ask Questions from Kubernets Experts.   Join Here    Join Meetup   Join Meetup and stay updated with Kubernetes Workshops , Events & Trainings.   Join Here    Complete Workshop & Challenge   Complete Workshop lab using Kubernetes Free Sandbox , or alternative . To get started with guided hands-on labs ,   Click Here    Attending Kubernetes Community Days Lahore   To avail the perks, you have to join the Kubernetes Community Days to get yourself registered   Click Here    Share a word about Kubernetes Bootcamp & Kubernetes Community Days Lahore on social media   Sharing post and telling people about Kubernetes Community Days can increase your points and ranking. Dont Forget to Tag Kubernetes Community Day's Lahore.    Fill the Google Form   After Completing all the above requirements , you have to fill the google form   Click Here",{"id":12,"path":13,"dir":14,"title":15,"description":16,"keywords":17,"body":18},"content:1.introduction:1.getting-started.md","/introduction/getting-started","introduction","Before We Begin","I'd like to share a story with you—a tale of the Kubernetes Community Days. This marks our second event, unfolding once again in Lahore, made possible by your invaluable support and the contributions of many to this vibrant ecosystem. As a team, we've poured our hearts and souls into this endeavor, driven by a fervent desire to empower our local community with knowledge and resources on Kubernetes. Before the inception of the Kubernetes Community Days, we realized a significant gap: many, including professionals, were unfamiliar with \"Kubernetes.\" Mentioning the name often elicited puzzled looks and curious questions: \"Kubernetes? What's that?\"",[],"  Before We Begin  I'd like to share a story with you—a tale of the Kubernetes Community Days. This marks our second event, unfolding once again in Lahore, made possible by your invaluable support and the contributions of many to this vibrant ecosystem. As a team, we've poured our hearts and souls into this endeavor, driven by a fervent desire to empower our local community with knowledge and resources on Kubernetes. Before the inception of the Kubernetes Community Days, we realized a significant gap: many, including professionals, were unfamiliar with \"Kubernetes.\" Mentioning the name often elicited puzzled looks and curious questions: \"Kubernetes? What's that?\"  Driven by our realization, we, a group of engineers, ventured into the realm of marketing. Our goal was to illuminate the benefits of Kubernetes, highlight job opportunities, and encourage contributions to the ecosystem. Our marketing efforts were creative and informative; we used memes to demystify Kubernetes and crafted posts and reels that significantly raised awareness. We reached out to potential sponsors, receiving mixed responses, but thankfully, companies like GitHub and Google Cloud stepped forward. Their support enabled us to host a high-quality, free event, complete with meals for all attendees at the Kubernetes Community Days Lahore 2023 edition.  Our efforts bore fruit, attracting over 300 developers to the University of Central Punjab and introducing them to Kubernetes technologies—a monumental achievement for our local community.  Post-event, we identified challenges and strategized solutions for the upcoming year. A notable issue was the accessibility of workshops; the cost of Sandbox labs, especially those based in the US, was prohibitive for many. Alternatives like Minikube or Kubeadm required intricate setups that could take hours. To address this, we've innovated a solution: a new, cost-free Sandbox. This tool enables anyone to set up a Kubernetes lab effortlessly. In the following section, we'll guide you on how to activate and make the most of your Sandbox.",{"id":20,"path":21,"dir":14,"title":22,"description":23,"keywords":24,"body":27},"content:1.introduction:2.Active-your-sandbox.md","/introduction/active-your-sandbox","Activate Your Sandbox","Before you begin, make sure you meet the following requirements:",[25,26],"Activate Your Kubernetes Playground (Cloud)","Integrate Kubernetes Playground Locally!","  Activate Your Sandbox  Before you begin, make sure you meet the following requirements:    GitHub Account : If you do not have a GitHub account, please   create a free account  before starting with the sandbox. This is essential for accessing the Kubernetes Sandbox environment.  Please ensure that no additional tabs are open to guarantee optimal performance. It is also important to verify that you have a stable internet connection.  We highly recommend using Google Chrome.  Activate Your Kubernetes Playground (Cloud)  To dive into the hands-on experience with Kubernetes, activate your Kubernetes Sandbox using GitHub Codespaces by clicking the button below:  Kubernetes Sanbox    Sandbox creation usually takes 2-3 minutes. In case it takes more time, please refresh your session.  Once the terminal is ready, please enter the following command so the Kubernetes cluster and node can be completely set up.     make   create\n  Integrate Kubernetes Playground Locally!  To explore the Kubernetes Playground locally, please click the link below:  kcdctl  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":29,"path":30,"dir":14,"title":31,"description":32,"keywords":33,"body":37},"content:1.introduction:3.what-you-will-learn.md","/introduction/what-you-will-learn","What You Will Learn","By the end of this bootcamp, you will have a solid understanding of what Kubernetes is and what it does. Specifically, you will learn how to:",[34,35,36],"Why Kubernetes?","Workshop Modules","Ready to Start?","  What You Will Learn  By the end of this bootcamp, you will have a solid understanding of what Kubernetes is and what it does. Specifically, you will learn how to:   Deploy, scale, update, and debug containerized applications on a Kubernetes cluster.  Utilize an interactive online terminal for real-world, practical application of your knowledge.  Why Kubernetes?  In today's digital landscape, users expect applications to be available 24/7, and developers are under pressure to deploy new versions of these applications multiple times a day. Kubernetes is designed to support this dynamic environment by enabling:    Continuous Deployment and Scaling : Release and update applications quickly and without downtime.   Efficient Resource Utilization : Intelligent scaling and resource management to meet user demand.   Reliability and Availability : Ensure that applications are always available to users.  Kubernetes combines Google's deep experience in running production loads at scale with the best-of-breed ideas and practices from the community.  Workshop Modules  Throughout this bootcamp, you will complete the following modules:     Create a Cluster : Learn the basics of Kubernetes clusters and how to set one up.    Deploy an App : Deploy your first application on Kubernetes.    Explore Your App : Understand how to inspect and interact with your deployed application.    Expose Your App Publicly : Make your application accessible from the internet.    Scale Up Your App : Adjust your application's resources to handle increased traffic.    Update Your App : Safely roll out updates to your application.  Ready to Start?  With everything set up and your understanding of the bootcamp structure, you're now ready to embark on your Kubernetes learning journey. Let's dive in and explore the powerful capabilities of Kubernetes together!",{"id":39,"path":40,"dir":14,"title":41,"description":42,"keywords":43,"body":45},"content:1.introduction:4.you-must-know.md","/introduction/you-must-know","Workshop Terms and Conditions and Usage Guidelines","Before proceeding with the workshop, it is crucial that all participants read, understand, and agree to the following terms and conditions and usage guidelines. Your participation in this workshop indicates your acceptance of these terms.",[44],"General Terms","  Workshop Terms and Conditions and Usage Guidelines  Before proceeding with the workshop, it is crucial that all participants read, understand, and agree to the following terms and conditions and usage guidelines. Your participation in this workshop indicates your acceptance of these terms.  General Terms    Limited Vouchers : We have a limited number of vouchers available for this workshop. Only participants who are passionate and complete all the required tasks and activities will be officially contacted and eligible for these vouchers.   Proper Use of Resources :   Abuse of the Kubernetes sandbox, including but not limited to, using it for hosting a personal website, mining for cryptocurrency, or engaging in hacking activities, will result in immediate termination of your services.  Participants must agree to the GitHub Codespaces terms and conditions. In the event of your GitHub account or service being terminated, CNCF Lahore & Team will not be involved or responsible for resolving such issues.   Respectful Behavior :   All participants are expected to conduct themselves in a professional and respectful manner at all times. Harassment, bullying, or discrimination of any kind will not be tolerated.  Any reports of inappropriate behavior will be taken seriously and may result in immediate expulsion from the workshop.   Confidentiality :   Some information and materials provided during the workshop may be confidential or proprietary. Participants agree not to disclose such information without explicit permission.   Giveaway Terms :   CNCF Lahore reserves the right to cancel the giveaway of vouchers, swags, and LinkedIn Premium subscriptions if a participant is found not to have completed all the requirements.  Shortlisted candidates will be contacted for a one-on-one call to ensure that the items are not sold or transferred to any third party. This measure is in place to maintain the integrity of our giveaway and ensure that rewards reach genuine participants who have earnestly contributed to and benefited from the workshop.",{"id":47,"path":48,"dir":49,"title":50,"description":7,"keywords":51,"body":56},"content:2.introduction-to-cluster:1.Creating-Cluster.md","/introduction-to-cluster/creating-cluster","introduction-to-cluster","Introduction to Kubernetes!",[52,53,54,55],"What is Kubernetes?","Understanding the Kubernetes Cluster","Getting Started","Ready, Set, Go!","  Introduction to Kubernetes!  What is Kubernetes?  Kubernetes, often abbreviated as K8s (with \"8\" representing the eight letters that are omitted), is an open-source platform designed to automate deploying, scaling, and operating application containers. It groups containers that make up an application into logical units for easy management and discovery. Kubernetes is built to run across a cluster of machines, providing a high level of availability.  Understanding the Kubernetes Cluster  A Kubernetes cluster consists of two main components:    Master : The brain behind the operation, coordinating the cluster.   Nodes : The workers, running your applications.  Cluster Diagram    This diagram illustrates how the master and nodes interact within a Kubernetes cluster.  Dive Deeper into the Components  The Master  The master is responsible for managing the cluster. It makes decisions about where to run applications based on resource availability and other scheduling criteria. It also handles scaling and updates to ensure the cluster operates smoothly.  Nodes  A node can be a virtual or physical machine, serving as the workhorse that runs your applications. Each node has the necessary tools to manage container operations, such as Docker, and is managed by the master. Nodes communicate with the master via the Kubernetes API, which the master exposes.  Why Kubernetes?  Kubernetes abstracts the hardware infrastructure, allowing applications to run on any node within the cluster without needing to be tied to specific machines. This flexibility significantly simplifies deploying, managing, and scaling applications.  Getting Started  For development purposes,   minikube  is a recommended tool. It simplifies the process of setting up a Kubernetes cluster by creating a VM on your local machine that hosts a single-node cluster. Minikube supports Linux, macOS, and Windows, offering an accessible way to start with Kubernetes.   Guess what we are using MiniKube with GitHubCodespaces for Kubernetes Sandbox.  The Name \"Kubernetes\"  Originating from Greek, Kubernetes means \"helmsman\" or \"pilot,\" highlighting its role in guiding and managing containerized applications.  Ready, Set, Go!  Now that you have a basic understanding of Kubernetes and its components, it's time to dive into the practical side.",{"id":58,"path":59,"dir":49,"title":60,"description":61,"keywords":62,"body":66},"content:2.introduction-to-cluster:2.Lab.md","/introduction-to-cluster/lab","Lab01 : Creating Your First Cluster","Creating your first cluster with a basic experience can be a bit challenging. Also, it is very dependent on your machine's hardware, because you need good RAM and memory to get started with Minikube or Kubeadm installation. If you have already activated your sandbox, please skip this lab. In case you are still struggling, then this is a step-by-step guide to help you activate your first cluster without a cost and depending on your hardware.",[63,64,65],"Activate Your Kubernetes Playground (Cloud Solution)","Creating a Cluster","Checking the Node","  Lab01 : Creating Your First Cluster  Creating your first cluster with a basic experience can be a bit challenging. Also, it is very dependent on your machine's hardware, because you need good RAM and memory to get started with Minikube or Kubeadm installation. If you have already activated your sandbox, please skip this lab. In case you are still struggling, then this is a step-by-step guide to help you activate your first cluster without a cost and depending on your hardware.  Before you begin, make sure you meet the following requirements:    GitHub Account : If you do not have a GitHub account, please   create a free account  before starting with the sandbox. This is essential for accessing the Kubernetes Sandbox environment.  Please ensure that no additional tabs are open to guarantee optimal performance. It is also important to verify that you have a stable internet connection.  We highly recommend using Google Chrome.  Activate Your Kubernetes Playground (Cloud Solution)  If you already have an active Kubernetes Playground, please skip this step. In case you don't, please click the button below:  Kubernetes Sanbox   While conducting experiments, I found that Codespaces sometimes became slow. This issue was related to low memory and CPU, as well as poor internet connections. If you encounter any similar issues, try to ensure you have a good connection or consider refreshing your Codespace session.  Creating a Cluster  After opening the terminal and ensuring your connection is stable, you can initiate the creation of a control plane and node by entering the following command:     make   create\n  Checking the Node  After successfully executing the   make create  command, you can verify the status of the node by using the following command:     kubectl   get   nodes\n  This command will provide an output similar to the example below, showing the status, roles, age, and version of your nodes:     @adilshehzad786 ➜ /workspaces/Kubernetes-Playground (main) $ kubectl get nodes\n   NAME                       STATUS   ROLES                  AGE    VERSION\n   k3d-k3s-default-server-0   Ready    control-plane,master   3m6s   v1.21.3+k3s1\n  This output indicates that your Kubernetes node is ready and functioning correctly within the specified roles and version.  Upon execution, the process should resemble the screenshot below:      Congratulations, you have successfully completed Lab 01. Now it's time to deploy your first app.  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":68,"path":69,"dir":70,"title":71,"description":72,"keywords":73,"body":76},"content:3.Deploy-an-App:1.Your-First-App.md","/deploy-an-app/your-first-app","deploy-an-app","Deploy Your First Application on Kubernetes","Welcome to the exciting journey of deploying your first application on a Kubernetes cluster! If you've ever wondered how modern applications ensure reliability and scalability, you're about to dive into one of the core practices that make it possible.",[74,75],"Understanding Deployments in Kubernetes","Let's Deploy Your First App!","  Deploy Your First Application on Kubernetes  Welcome to the exciting journey of deploying your first application on a Kubernetes cluster! If you've ever wondered how modern applications ensure reliability and scalability, you're about to dive into one of the core practices that make it possible.  Understanding Deployments in Kubernetes  Think of a   Deployment  in Kubernetes as your personal assistant for managing your application. It takes care of starting your app, making sure it's running on the cluster, and stepping in to fix things if something goes wrong. This is a big leap from the old days when applications were manually started, and if something failed, well, it was up to you to notice and fix it.  Deployments automate the process of managing your application instances across the cluster's Nodes (the machines running your app). If a Node encounters a problem, the Deployment is like a vigilant guardian that quickly replaces the affected instance, ensuring your service remains available.  The Self-Healing Mechanism  This self-healing mechanism is a game-changer. It means that your application can recover from hardware failures, maintenance, and other disruptions automatically. Deployments keep your app resilient and available, letting you sleep well at night knowing your app can take care of itself.  Let's Deploy Your First App!  Deploying an application on Kubernetes involves packaging your app into a container and telling Kubernetes how to run it. We'll use a simple NodeJS application as our example, but the concepts apply to any application you might want to deploy.  What You'll Need:    Kubectl : This is the command-line tool that lets you communicate with your Kubernetes cluster. Think of it as your bridge to the Kubernetes world.   This tool is already configured to your Sandbox   A Containerized Application : Your app needs to be packaged in a container format supported by Kubernetes. For our example, we'll use a Docker container.  The Application  We have already configured an image for you; feel free to use it:   docker pull ghcr.io/cncf-lahore/nodejs-app:latest\n  Deploying the App    Prepare Your Application : Make sure your application is containerized and ready to go. If you're following our example, clone the GitHub repository to get the Dockerfile and source code.   Use Kubectl : With your application containerized, you'll use Kubectl to create a Deployment on your Kubernetes cluster. This process tells Kubernetes how to run your app, how many instances you want, and how to manage them.  Next Steps  Now that you have a basic understanding of what a Deployment is and how it works, it's time to put this knowledge into action. Head over to the tutorial to deploy your first app on Kubernetes. Follow the steps, and you'll see your application come to life on your very own Kubernetes cluster.  Happy deploying! 🚀",{"id":78,"path":79,"dir":70,"title":80,"description":7,"keywords":81,"body":82},"content:3.Deploy-an-App:2.Lab.md","/deploy-an-app/lab","Lab02 : Deploy An App",[],"  Lab02 : Deploy An App   Once you activate your Sandbox, the next step is to deploy an App:   Clone the Repository :   Navigate to the deployment folder:\n     cd   deploy\n  Clone the repository containing the Node.js application:\n     git   clone   https://github.com/CNCF-Lahore/nodejsapp.git\n   Deploy the Application :   Change directory to the cloned repository:\n     cd   nodejsapp\n  Apply the deployment configuration to your Kubernetes cluster:\n     kubectl   apply   -f   deployment.yaml\n  Apply the service configuration to expose the application:\n     kubectl   apply   -f   service.yaml\n   Verify the Deployment :   Check the status of the deployed pods to ensure they are running:\n     kubectl   get   pod\n    Congratulations, you have successfully completed Lab 02. Now it's time to explore the Node.js app which you deployed on Kubernetes.  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":84,"path":85,"dir":86,"title":87,"description":7,"keywords":88,"body":91},"content:4.Explore-Your-App:1.how-to-explore.md","/explore-your-app/how-to-explore","explore-your-app","Explore Your App",[89,90],"Pods","Nodes","  Explore Your App  Pods  In Module 2, the Deployment process creates a   Pod , which hosts your application instance. A Pod is essentially a group of one or more application containers (such as Docker or rkt), including shared storage (volumes), a unique cluster IP address, and information about how to run them (e.g., container image version or specific ports). Containers within a Pod share an IP address and port space, are always co-located and co-scheduled, and run in a shared context on the same node.  This setup models an application-specific “logical host” and contains one or more application containers that are relatively tightly coupled. For instance, alongside a NodeJS app container, a side container that feeds the data published by the webserver might be deployed within the same Pod. Prior to the era of containers, such applications would have run on the same physical or virtual machine.  Pods remain tied to the Node where they are deployed until they are terminated (according to their restart policy) or deleted. In the event of a Node failure, identical Pods are redeployed on other available Nodes. The Pod is considered the atomic deployment unit on the Kubernetes platform.    Nodes   Nodes  are the worker machines in Kubernetes, which can be either VMs or physical machines, depending on the cluster. Each Node is responsible for running Pods and is managed by the Master. The Master automatically schedules Pods based on the available resources on the Nodes.  Every Kubernetes Node runs at least:   A   container runtime  (such as containerd or CRI-O, not Docker as of recent Kubernetes versions) to pull and manage containers from a registry.   Kubelet , acting as a bridge between the Kubernetes Master and the Nodes, managing the Pods and the containers running on a machine.    Kubernetes and Container Runtimes  As of Kubernetes version 1.20, direct support for Docker as a container runtime has been deprecated due to Kubernetes' use of the Container Runtime Interface (CRI). The CRI allows Kubernetes to use a variety of container runtimes without needing to integrate them directly into the Kubernetes codebase. This move towards more flexible container runtime options enables Kubernetes to leverage runtimes like   containerd  and   CRI-O , which are designed to be simpler and more efficient than Docker for the specific needs of Kubernetes.",{"id":93,"path":94,"dir":86,"title":95,"description":96,"keywords":97,"body":98},"content:4.Explore-Your-App:2.Lab.md","/explore-your-app/lab","lab03: Exploring Pods and Nodes","When working with Kubernetes, managing and exploring Pods and Nodes is crucial. Here's a quick guide on using kubectl commands for these tasks:",[],"  lab03: Exploring Pods and Nodes  When working with Kubernetes, managing and exploring Pods and Nodes is crucial. Here's a quick guide on using kubectl commands for these tasks:   Pods :    Listing Pods Across All Namespaces : To see every Pod running across all namespaces in your cluster, use:\n     kubectl   get   pods   --all-namespaces\n   Listing Pods Within a Specific Namespace : If you want to focus on Pods within a particular namespace, modify your command like so:\n     kubectl   get   pods   -n   \u003C  namespac  e  >\n   Describing a Pod : For detailed information about a specific Pod, including its status and events, use the describe command:\n     kubectl   describe   pod   \u003C  pod-nam  e  >   -n   \u003C  namespac  e  >\n   Nodes :    Listing All Nodes : To get a list of all Nodes in your cluster, along with their status, use:\n     kubectl   get   nodes\n   Describing a Node : For a deep dive into a specific Node's details, including the Pods running on it and its available resources, use:\n     kubectl   describe   node   \u003C  node-nam  e  >\n  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":100,"path":101,"dir":102,"title":103,"description":104,"keywords":105,"body":110},"content:5.Expose-Your-App:1.How-to-make-it-public.md","/expose-your-app/how-to-make-it-public","expose-your-app","Introduction to Services","In Kubernetes, while Pods are assigned their own unique IP addresses within the cluster, these IPs are not accessible from outside the Kubernetes environment. Given the dynamic nature of Pods—being terminated, deleted, or replaced—there needs to be a mechanism for continuous and automatic discovery among Pods and applications. This is where Kubernetes Services come into play, acting as an abstraction layer that groups Pods and allows for their exposure to external traffic, load balancing, and service discovery.",[106,107,108,109],"Key Concepts of Services","Types of Services","Exposing Pods Using Services","Using Labels for Organization and Selection","  Introduction to Services  In Kubernetes, while Pods are assigned their own unique IP addresses within the cluster, these IPs are not accessible from outside the Kubernetes environment. Given the dynamic nature of Pods—being terminated, deleted, or replaced—there needs to be a mechanism for continuous and automatic discovery among Pods and applications. This is where   Kubernetes Services  come into play, acting as an abstraction layer that groups Pods and allows for their exposure to external traffic, load balancing, and service discovery.  Key Concepts of Services    External Traffic Exposure : Services enable the exposure of Pods to traffic coming from outside the cluster.   Load Balancing : Services distribute incoming traffic among all the Pods that match its configuration, ensuring high availability and reliability.   Service Discovery : Services allow for the automatic discovery of Pods within the cluster, facilitating communication between different services, like a frontend webserver and a backend database.  Types of Services    LoadBalancer : Offers a public IP address for the service, making it accessible from outside the Kubernetes cluster. This is commonly used in cloud environments like Google Kubernetes Engine (GKE) or AWS.   NodePort : Exposes the service on a specific port across all Nodes of the cluster using Network Address Translation (NAT). This type is available on all Kubernetes clusters, including Minikube, and is useful for development and testing environments.    Exposing Pods Using Services  To expose your Pods to the outside world, you can create a Service. There are several ways to do this, depending on your requirements and the environment in which your Kubernetes cluster is running.    In Lab03, you don't need to expose an app; we already did it for you, as we are using NodePort to expose the app.  Creating a Service  You can expose your application by creating a Service. This is done using the   kubectl  command line. For example, to create a NodePort Service, you would use:     kubectl   expose   deployment   \u003C  deployment-nam  e  >   --type=NodePort   --name=  \u003C  service-name  >\n  This command creates a Service that exposes your Deployment on a specific port on each Node of the cluster.  Finding the Service IP and Port  After creating your Service, you can find the IP address and the port exposed by the Service using:     kubectl   get   services/  \u003C  service-nam  e  >\n  This will list the details of your service, including the NodePort assigned to your service if you've used the NodePort type.  Port Forwarding (Without Ingress or Load Balancer)  In cases where you don't have an Ingress controller or a LoadBalancer available, you can use port forwarding to access your application:     kubectl   port-forward   services/  \u003C  service-nam  e  >   \u003C  local-por  t  >  :  \u003C  service-por  t  >\n  This command forwards traffic from a local port on your machine to the service port on the cluster, allowing you to access the application via   localhost:\u003Clocal-port> .  Using Labels for Organization and Selection  Labels are key/value pairs attached to Kubernetes objects, like Pods, which can be used for organizing and selecting subsets of objects.  Attaching labels to Kubernetes objects and filtering those objects based on labels are powerful features that help manage and organize resources within a cluster. Here’s a breakdown of how to do both:   Attaching Labels  Labels are key-value pairs that can be attached to Kubernetes objects, such as Pods, Deployments, and Services. They allow you to organize and select subsets of objects based on these labels. You can attach labels at the time of creation or add them to existing objects.   At Creation Time : You can specify labels when creating an object using the   kubectl  command by including the   --labels  (or   -l  for short) option. For example, to create a Pod with specific labels, you might use:     kubectl   run   nginx   --image=nginx   --labels=environment=production,app=nginx\n  This command creates an nginx Pod with labels   environment=production  and   app=nginx .   Adding to Existing Objects : To add labels to existing Kubernetes objects, you can use the   kubectl label  command. For example, to add a label to a Pod, you would use:     kubectl   label   pods   \u003C  pod-nam  e  >   environment=dev\n  This command adds the label   environment=dev  to the specified Pod. If the label already exists, you need to add the   --overwrite  flag to change its value.   Selecting Pods with Services Using Labels  Services in Kubernetes use label selectors to determine which Pods to include in the service. This makes it possible for a Service to dynamically select Pods based on their labels, allowing for loose coupling between Pods and the Services that expose them.  When defining a Service, you specify a selector that matches the labels of the Pods you want the Service to manage. Here is an example Service definition that selects Pods based on their labels:     apiVersion  :   v1\n   kind  :   Service\n   metadata  :\n     name  :   frontend-service\n   spec  :\n     selector  :\n       app  :   frontend\n     ports  :\n       -   protocol  :   TCP\n         port  :   80\n         targetPort  :   8080\n  In this example, the Service   frontend-service  will select any Pod with the label   app=frontend  and route traffic to port   8080  on those Pods.   Filtering Using Labels  You can filter the output of   kubectl get  commands by label. This is useful for quickly finding resources that match specific criteria. For example, to list all Pods with a particular label:     kubectl   get   pods   -l   environment=production\n  This command lists all Pods that have the label   environment=production .  Using labels and label selectors effectively can greatly simplify the management of resources in a Kubernetes cluster, allowing for more flexible and dynamic configurations.    html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":112,"path":113,"dir":102,"title":114,"description":115,"keywords":116,"body":118},"content:5.Expose-Your-App:2.Lab.md","/expose-your-app/lab","Lab 04: Exposing Your Application","In this lab, we will deploy a sample application to a Kubernetes cluster. The application will be automatically exposed using NodePort. NodePort is a simple way to make your application accessible from outside the Kubernetes cluster. It is primarily used for testing purposes and is not recommended for production environments.",[117],"Deployment Instructions","  Lab 04: Exposing Your Application  In this lab, we will deploy a sample application to a Kubernetes cluster. The application will be automatically exposed using NodePort. NodePort is a simple way to make your application accessible from outside the Kubernetes cluster. It is primarily used for testing purposes and is not recommended for production environments.  Deployment Instructions    Deploy the Application:   Navigate to the main directory of the application by entering the root path   /  in your terminal.  Deploy the application to the cluster by running the following command:\n     make   deploy\n   Access the Deployed Services: \nOnce the deployment is complete, several services including Zipkin, Swagger, Prometheus, Grafana, and a Node.js application will be running and exposed on various ports. To explore these services, follow the instructions below:    Zipkin:   Zipkin is used for tracing network requests within your application.  To access Zipkin, check the ports section in your terminal and navigate to the port   9411 .  To try out Zipkin with sample data, download the following JSON file and upload it to Zipkin:\n   https://github.com/openzipkin/zipkin/blob/master/zipkin-lens/testdata/ascend.json\n   Prometheus:   Prometheus is used for monitoring the application. It allows you to see which APIs are currently down.  To access Prometheus, navigate to port   30000 .  In Prometheus, go to the \"Targets\" section to view the status of various APIs. For this lab, some APIs are intentionally set to be down and will be brought up later.   Grafana:   Grafana is used for visualizing metrics through graphs.  To access Grafana, navigate to port   32000 .  Resetting Grafana Admin Password  If you encounter issues logging into Grafana with the expected credentials, for learning purposes, you can reset the admin password using the following steps:   Retrieve the name of the Grafana pod:     kubectl   get   pods   -n   monitoring\n  Exec into the Grafana pod:     kubectl   exec   -it   \u003C  grafana-pod-nam  e  >   -n   monitoring   --   /bin/sh\n  Reset the admin password:     grafana-cli   admin   reset-admin-password   newpassword\n  your username is   admin  and password is   newpassword  Importing a Prometheus Dashboard into Grafana  To visualize Prometheus metrics in Grafana, you can import pre-configured dashboards. Here’s how to import a Prometheus dashboard:    Access Grafana:  Log in to your Grafana instance using the admin credentials.   Open the Dashboard Import Interface:   From the Grafana main menu, select the \"+\" icon on the left sidebar.  Choose \"Import\".   Import Dashboard:   You can import a dashboard using a dashboard ID from Grafana.com Dashboards, or by uploading a JSON file.  To import a dashboard from Grafana.com, enter the dashboard ID (e.g.,   1860  for a popular Node Exporter dashboard) into the “Import via grafana.com” field and click \"Load\".  Alternatively, if you have a dashboard JSON file, click \"Upload JSON file\" and select the file from your computer.   Select Prometheus as the Data Source:   During the import process, you will be asked to select a data source.  Choose \"Prometheus\" from the dropdown menu.   Finalize Import:   After selecting the data source, click \"Import\" to complete the process.  You should now see the imported dashboard with metrics from your Prometheus data source.  Now we learn how to expose the apps. In the next section, we will learn how we can scale a Node.js application.  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":120,"path":121,"dir":122,"title":123,"description":124,"keywords":125,"body":129},"content:6.Scale-Your-App:1.scale-In-and-Out.md","/scale-your-app/scale-in-and-out","scale-your-app","Understanding Scaling","In this module, we delve into scaling your application within a Kubernetes environment. Scaling is a critical aspect of managing applications, ensuring they can handle varying loads of traffic. Kubernetes simplifies this process through Deployments and Services, enabling both scale-out (increasing the number of replicas) and scale-in (decreasing the number of replicas) operations.",[126,127,128],"Architecture of Scaling","How to Scale Your Application","Autoscaling (Beyond the Basics)","  Understanding Scaling  In this module, we delve into scaling your application within a Kubernetes environment. Scaling is a critical aspect of managing applications, ensuring they can handle varying loads of traffic. Kubernetes simplifies this process through Deployments and Services, enabling both scale-out (increasing the number of replicas) and scale-in (decreasing the number of replicas) operations.  Scaling in Kubernetes is primarily managed by adjusting the number of replicas in a Deployment. A replica represents an instance of your application running on a Pod. By increasing the number of replicas, you can scale out to handle more traffic. Conversely, reducing the number of replicas will scale in your application, reducing resources used when demand is lower.  Architecture of Scaling    Scaling Out (Up) : Creates new Pods and schedules them to Nodes with available resources, ensuring your application can handle increased load.   Scaling In (Down) : Removes Pods, reducing the number of instances to match the desired state, conserving resources.  Kubernetes Services play a crucial role in this process. A Service acts as a load balancer, distributing incoming traffic across all Pods of a Deployment. It continuously monitors the running Pods through endpoints, directing traffic only to available Pods, ensuring efficient load distribution and high availability.  How to Scale Your Application  Creating a Deployment with Multiple Replicas  You can specify the number of replicas at the creation of a Deployment:     kubectl   create   deployment   \u003C  deployment-nam  e  >   --image=  \u003C  image-name  >   --replicas=  \u003C  number-of-replicas  >\n  Scaling an Existing Deployment  To adjust the number of replicas for an existing Deployment, use the   kubectl scale  command:    Scaling Out : Increase the number of replicas     kubectl   scale   deployment   \u003C  deployment-nam  e  >   --replicas=  \u003C  desired-number-of-replicas  >\n   Scaling In : Decrease the number of replicas     kubectl   scale   deployment   \u003C  deployment-nam  e  >   --replicas=  \u003C  desired-number-of-replicas  >\n  Checking the Status of the Scaling Process  To monitor the status of your Deployment and the number of running replicas, use:     kubectl   get   deployments   \u003C  deployment-nam  e  >\n  This command provides information about the Deployment, including the desired and current number of replicas.  Autoscaling (Beyond the Basics)  While manual scaling provides control over the number of replicas, Kubernetes also supports autoscaling. This feature automatically adjusts the number of Pod replicas based on CPU usage or other metrics, although it's beyond the scope of this basic tutorial. Autoscaling ensures that your application has the resources it needs when demand spikes, without manual intervention. \n\n\n\n    Slide Presentation  \n  .slide-container {\n    display: flex;\n    overflow-x: scroll;\n    scroll-snap-type: x mandatory;\n    -webkit-overflow-scrolling: touch;\n    gap: 20px; /* Adds space between slides */\n    padding: 20px; /* Adds some padding around slides */\n    box-sizing: border-box;\n  }\n  .slide-container::-webkit-scrollbar {\n    display: none; /* Hides scrollbar for webkit browsers */\n  }\n  .slide {\n    flex: 0 0 auto;\n    width: 100%;\n    scroll-snap-align: start;\n    border-radius: 10px; /* Optional: rounds corners of slides */\n    box-shadow: 0 4px 6px rgba(0,0,0,0.1); /* Optional: adds shadow for depth */\n    transition: transform 0.5s ease-in-out; /* Smooth transition for scaling */\n  }\n  .slide:hover {\n    transform: scale(1.05); /* Scales up slide on hover for a dynamic effect */\n  }\n  img {\n    max-width: 100%;\n    height: auto;\n    display: block;\n    border-radius: 10px; /* Ensures the img fits the slide's rounded corners */\n  }\n \n\n\n  \n    \n      \n   \n    \n      \n   \n\n  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":131,"path":132,"dir":122,"title":133,"description":134,"keywords":135,"body":138},"content:6.Scale-Your-App:2.Lab.md","/scale-your-app/lab","Lab 05: Scaling Your Application","Welcome to Lab 05! In this session, we'll learn how to efficiently scale your applications using Kubernetes. We'll explore both declarative and imperative methods to give you the tools and understanding needed to manage your app's scalability like a pro.",[136,137],"Scaling with the Declarative Approach","Scaling with the Imperative Method","  Lab 05: Scaling Your Application  Welcome to Lab 05! In this session, we'll learn how to efficiently scale your applications using Kubernetes. We'll explore both declarative and imperative methods to give you the tools and understanding needed to manage your app's scalability like a pro.  Scaling with the Declarative Approach  The declarative approach is about telling Kubernetes what you want your application's infrastructure to look like, without having to detail every step to get there. This method aligns with the Infrastructure as Code (IaC) philosophy, allowing you to manage your infrastructure through version-controlled configuration files. This ensures transparency, reproducibility, and consistency.  How to Scale Up:  Imagine our Node.js application is running smoothly with two replicas, but we need more horsepower to handle increasing load. Here's how we can scale up:   First, make sure you're in the Node.js app directory. If it's not on your machine, grab it from   Lab 02 .     cd   deploy/nodejsapp\n  Open the   deployment.yaml  file and change the   replicas  value from 2 to 3. Your file should now look something like this:     apiVersion  :   apps/v1\n   kind  :   Deployment\n   metadata  :\n     name  :   nodejs-app\n   spec  :\n     replicas  :   3\n     selector  :\n       matchLabels  :\n         app  :   nodejs-app\n     template  :\n       metadata  :\n         labels  :\n           app  :   nodejs-app\n       spec  :\n         containers  :\n         -   name  :   nodejs-app\n           image  :   ghcr.io/cncf-lahore/nodejs-app:latest\n           ports  :\n           -   containerPort  :   80\n  Apply the changes to scale up your deployment:     kubectl   apply   -f   deployment.yaml\n    Scaling with the Imperative Method  The imperative method is more hands-on. You directly command the Kubernetes cluster to change your app's resources in real-time. It's like giving step-by-step instructions without defining the final state ahead of time.  How It Works:  If you're curious to try the imperative method, start by removing the deployment created through the declarative method:     kubectl   delete   -f   deployment.yaml\n  Next, create a new deployment with three replicas directly:     kubectl   create   deployment   nodejsapp   --image=ghcr.io/cncf-lahore/nodejs-app:latest   --replicas=3\n    To scale down in the future, use:     kubectl   get   deployment\n   kubectl   scale   deployment   nodejsapp   --replicas=2\n       Congratulations!  You've mastered scaling your applications up and down using both declarative and imperative methods. Ready for the next challenge? Let's move on to learning how to roll out updates without downtime in the final lab.  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":140,"path":141,"dir":142,"title":143,"description":144,"keywords":145,"body":153},"content:7.Update-Your-App:1.Rolling-up.md","/update-your-app/rolling-up","update-your-app","Understanding Rolling Updates","Rolling updates are a fundamental feature in Kubernetes, enabling you to update the running version of your application seamlessly and without downtime. This approach is crucial for maintaining continuous availability, meeting the modern expectations of both users and developers for frequent updates.",[146,147,148,149,150,151,152],"Architecture and Workflow","Requirements for Zero Downtime","Rolling Update Commands","Checking the Rollout Status","Rolling Back an Update","Strategies for Zero Downtime Deployments","Summary","  Understanding Rolling Updates  Rolling updates are a fundamental feature in Kubernetes, enabling you to update the running version of your application seamlessly and without downtime. This approach is crucial for maintaining continuous availability, meeting the modern expectations of both users and developers for frequent updates.  A rolling update in Kubernetes incrementally replaces old Pods with new ones, which are based on a new version of the deployment image. This process ensures that the application remains available to users and that the update does not consume all resources, allowing for a balanced deployment.  Architecture and Workflow    Update Trigger : An update is triggered (usually by changing the Docker image version in a Deployment).   Pod Replacement : Kubernetes schedules new Pods with the new version while gradually terminating the old ones, ensuring that the service remains available throughout the process.   Resource Management : New Pods are only scheduled on Nodes with sufficient free resources, which optimizes resource use across the cluster.   Zero Downtime : Thanks to load balancing, users continue to access the service without interruption. The Service object in Kubernetes automatically redirects traffic to the available and updated Pods.  Requirements for Zero Downtime    Multiple Instances : Running multiple instances of your application is essential. This redundancy allows some Pods to be updated while others continue to serve user requests.   Proper Configuration : The Deployment must be properly configured to define the maximum number of Pods that can be unavailable and the maximum number of new Pods that can be created during the update.  Rolling Update Commands  Updating a Deployment  To perform a rolling update, you typically update the image of the Deployment. For example, to update an application to a new version, you use the following command:     kubectl   set   image   deployment/  \u003C  deployment-nam  e  >   \u003C  container-nam  e  >  =  \u003C  new-imag  e  >  :  \u003C  ta  g  >\n  This command sets a new image for the specified container within your Deployment. Kubernetes then starts a rolling update automatically.  Checking the Rollout Status  To monitor the status of the rollout, use:     kubectl   rollout   status   deployment/  \u003C  deployment-nam  e  >\n  This command provides real-time feedback on the progress of the update.  Rolling Back an Update  If something goes wrong, Kubernetes allows you to rollback to a previous state of the Deployment:     kubectl   rollout   undo   deployment/  \u003C  deployment-nam  e  >\n  This command reverts the Deployment to its previous state, leveraging Kubernetes' versioned update feature.  Strategies for Zero Downtime Deployments    Readiness Probes : Ensure your Pods have readiness probes configured. This makes Kubernetes only send traffic to Pods that are ready to handle requests.   Resource Limits : Define appropriate resource requests and limits to ensure that your containers have enough resources to run effectively but do not monopolize cluster resources.   Surge and Unavailability Settings : Customize the   maxSurge  and   maxUnavailable  settings in your Deployment configuration.   maxSurge  defines the maximum number of Pods that can be created above the desired number of Pods during an update.   maxUnavailable  defines the maximum number of Pods that can be unavailable during the update process.  Example Deployment Configuration for Rolling Updates  Here's a snippet of a Deployment manifest that specifies these parameters:     apiVersion  :   apps/v1\n   kind  :   Deployment\n   metadata  :\n     name  :   example-deployment\n   spec  :\n     replicas  :   3\n     strategy  :\n       type  :   RollingUpdate\n       rollingUpdate  :\n         maxSurge  :   1\n         maxUnavailable  :   1\n     selector  :\n       matchLabels  :\n         app  :   example\n     template  :\n       metadata  :\n         labels  :\n           app  :   example\n       spec  :\n         containers  :\n         -   name  :   example-container\n           image  :   example/image:v1\n           ports  :\n           -   containerPort  :   80\n  This configuration ensures a rolling update with at most one extra Pod beyond the desired count (  maxSurge ) and at most one Pod unavailable (  maxUnavailable ) at any time during the update process.  Summary  Rolling updates are a powerful feature of Kubernetes, allowing for continuous integration and delivery with zero downtime. By understanding and utilizing the concepts and commands outlined in this module, you can ensure that your applications remain available and responsive, even as you deploy updates and improvements. \n\n\n\n    Slide Presentation  \n  .slide-container {\n    display: flex;\n    overflow-x: scroll;\n    scroll-snap-type: x mandatory;\n    -webkit-overflow-scrolling: touch;\n    gap: 20px; /* Adds space between slides */\n    padding: 20px; /* Adds some padding around slides */\n    box-sizing: border-box;\n  }\n  .slide-container::-webkit-scrollbar {\n    display: none; /* Hides scrollbar for webkit browsers */\n  }\n  .slide {\n    flex: 0 0 auto;\n    width: 100%;\n    scroll-snap-align: start;\n    border-radius: 10px; /* Optional: rounds corners of slides */\n    box-shadow: 0 4px 6px rgba(0,0,0,0.1); /* Optional: adds shadow for depth */\n    transition: transform 0.5s ease-in-out; /* Smooth transition for scaling */\n  }\n  .slide:hover {\n    transform: scale(1.05); /* Scales up slide on hover for a dynamic effect */\n  }\n  img {\n    max-width: 100%;\n    height: auto;\n    display: block;\n    border-radius: 10px; /* Ensures the img fits the slide's rounded corners */\n  }\n \n\n\n  \n    \n      \n   \n    \n      \n   \n    \n      \n   \n    \n      \n   \n\n  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":155,"path":156,"dir":142,"title":157,"description":158,"keywords":159,"body":160},"content:7.Update-Your-App:2.Lab.md","/update-your-app/lab","Lab 06: Update Your Application","Welcome back! If you've been with us since Lab 01, you're already familiar with running two pods as part of our application deployment. But what happens when we need to update our application to a new version without causing downtime or service disruption? This scenario is common in projects where maintaining low latency, fault tolerance, and high availability is crucial. It's also a great opportunity to familiarize yourself with deployment strategies like blue/green and canary releases.",[],"  Lab 06: Update Your Application  Welcome back! If you've been with us since Lab 01, you're already familiar with running two pods as part of our application deployment. But what happens when we need to update our application to a new version without causing downtime or service disruption? This scenario is common in projects where maintaining low latency, fault tolerance, and high availability is crucial. It's also a great opportunity to familiarize yourself with deployment strategies like blue/green and canary releases.  In this lab, we'll be taking a practical approach to updating our application. Let's imagine we've tasked one of our developers with upgrading our Node.js application from version 1 to version 2.  Fortunately, we've prepared the new version of the app in advance to streamline the update process. To roll out the updated version of our Node.js app, execute the following commands:     kubectl   get   deployment\n   kubectl   set   image   deployment/nodejs-app   nodejs-app=ghcr.io/cncf-lahore/nodejs-app:v2\n  To monitor the progress of the rollout and ensure it's proceeding as expected, you can check the status using:     kubectl   rollout   status   deployment/nodejs-app\n   Oops!  There's been a hiccup. It seems the development team accidentally configured the application to use port 90 instead of the standard port 80, causing the application to become inaccessible. To quickly revert this change and restore the application to its previous state, you can use the following command:     kubectl   rollout   undo   deployment/nodejs-app\n  Just like that, our application is back online, running smoothly as if nothing happened—all within a matter of seconds.    This lab demonstrates the power of Kubernetes' rolling updates and rollbacks, allowing us to update and maintain our applications with minimal impact on availability. Stay tuned for more labs where we'll dive deeper into Kubernetes' features and best practices.    Congratulations, you have now completed all the labs. Please complete the quiz and let us know how we can improve the Bootcamp. Your feedback is highly appreciated..  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":162,"path":163,"dir":142,"title":164,"description":7,"keywords":165,"body":166},"content:7.Update-Your-App:3.Quiz.md","/update-your-app/quiz","Quiz",[]," ",{"id":168,"path":169,"dir":170,"title":171,"description":172,"keywords":173,"body":174},"content:8.Get-Your-Certificate:get-your-certificate.md","/get-your-certificate/get-your-certificate","get-your-certificate","Course of Completion Certificate","We hope you've learned a lot from the bootcamp. To share your achievement on social media, please generate your course completion certificate. Feel free to use the content below to share your success on social media",[],"  Course of Completion Certificate  We hope you've learned a lot from the bootcamp. To share your achievement on social media, please generate your course completion certificate. Feel free to use the content below to share your success on social media   Generate My Certificate  For the digital badge, please complete the quiz.   🚀 Exciting News: I've Completed the Kubernetes Bootcamp! 🚀\n\nThrilled to announce my completion of the Kubernetes Bootcamp with the KCD Lahore team's free sandbox. I've gained practical skills in app creation, deployment, scaling, and updates.\n\nCheck Out My Achievement! 🎉\n\n✨ See my Course of Completion Certificate and start your own Kubernetes journey : https://bit.ly/kubecamp\n\nLet’s inspire and learn from each other's tech journeys. Here's to our continued growth in the world of cloud computing!\n\n#Kubernetes #TechAchievement #LearnCoding #CloudComputing\n",{"id":176,"path":177,"dir":178,"title":179,"description":180,"keywords":181,"body":183},"content:9.More:1.make-cheatsheet-cluster.md","/more/make-cheatsheet-cluster","more","Sandbox Cluster Management Cheatsheet","This cheatsheet provides commands for managing a Sandbox cluster and deploying applications.",[182],"General Usage","  Sandbox Cluster Management Cheatsheet  This cheatsheet provides commands for managing a Sandbox cluster and deploying applications.  General Usage     make   all                # create a cluster and deploy the apps\n   make   create             # create a k3d cluster\n   make   delete             # delete the k3d cluster\n   make   deploy             # deploy the apps to the cluster\n   make   clean              # delete the apps from the cluster\n  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":185,"path":186,"dir":178,"title":187,"description":188,"keywords":189,"body":202},"content:9.More:2.More-Resources.md","/more/more-resources","Kubernetes Cheat Sheet (Updated for 2024)","A comprehensive cheat sheet for Kubernetes commands with the latest updates.",[190,191,192,193,194,195,196,197,198,199,200,201],"Kubectl Alias","Cluster Info","Contexts","Get Commands","Namespaces","Labels and Selectors","Describe and Delete Commands","Create vs Apply","Export YAML for New and Existing Objects","Logs and Debugging","Port Forward","Scaling and Autoscaling","  Kubernetes Cheat Sheet (Updated for 2024)  A comprehensive cheat sheet for Kubernetes commands with the latest updates.  Kubectl Alias  To make   kubectl  command shorter and faster to type, you can create an alias.   Linux     alias   k  =  kubectl\n   Windows PowerShell     Set-Alias   -  Name k   -  Value kubectl\n  Cluster Info    Get clusters     kubectl   config   get-clusters\n   Get cluster info     kubectl   cluster-info\n  Contexts  Contexts manage the connection parameters for clusters, namespaces, and authentication.    List all contexts     kubectl   config   get-contexts\n   Get the current context     kubectl   config   current-context\n   Switch the current context     kubectl   config   use-context   \u003C  context-nam  e  >\n   Set the default namespace for the current context     kubectl   config   set-context   --current   --namespace=  \u003C  namespace-name  >\n  For switching between contexts quickly, consider using   kubectx .  Get Commands  Common   get  commands to retrieve resources.     kubectl   get   all\n   kubectl   get   namespaces\n   kubectl   get   configmaps\n   kubectl   get   nodes\n   kubectl   get   pods\n   kubectl   get   rs\n   kubectl   get   svc   \u003C  service-nam  e  >\n   kubectl   get   endpoints   \u003C  endpoint-nam  e  >\n  Additional options:    -o wide : Show more information.   --watch  or   -w : Watch for changes in real-time.  Namespaces    Specify namespace for a command     kubectl   get   pods   --namespace=  \u003C  namespace-name  >\n  To switch namespaces for commands without specifying each time, consider using   kubens .  Labels and Selectors    Get pods showing labels     kubectl   get   pods   --show-labels\n   Filter pods by label     kubectl   get   pods   -l   environment=production,tier!=frontend\n   kubectl   get   pods   -l   'environment in (production,test),tier notin (frontend,backend)'\n  Describe and Delete Commands  Detailed information and deletion of resources.    Describe resource     kubectl   describe   \u003C  resource-typ  e  >   \u003C  resource-nam  e  >\n   Delete resource     kubectl   delete   \u003C  resource-typ  e  >   \u003C  resource-nam  e  >\n   Force delete a pod immediately     kubectl   delete   pod   \u003C  pod-nam  e  >   --grace-period=0   --force\n  Create vs Apply   kubectl apply  is recommended for most operations as it applies changes to resources while respecting existing configurations.    Create a deployment     kubectl   create   deployment   \u003C  nam  e  >   --image=  \u003C  image  >\n   Apply a configuration from a file     kubectl   apply   -f   \u003C  filename.yam  l  >\n  Export YAML for New and Existing Objects    Generate YAML for a new pod (dry run)     kubectl   run   \u003C  pod-nam  e  >   --image=  \u003C  image  >   --dry-run=client   -o   yaml   >   \u003C  pod-nam  e  >  .yaml\n   Export YAML of an existing object     kubectl   get   \u003C  resource-typ  e  >   \u003C  resource-nam  e  >   -o   yaml   >   \u003C  file-nam  e  >  .yaml\n  Logs and Debugging    Tail logs from pods     kubectl   logs   -f   \u003C  pod-nam  e  >\n   Get logs from a previously terminated container     kubectl   logs   \u003C  pod-nam  e  >   --previous\n  Port Forward  Directly access applications or services from your local machine.     kubectl   port-forward   \u003C  type/nam  e  >   \u003C  local-por  t  >  :  \u003C  pod-por  t  >\n  Scaling and Autoscaling    Manually scale a deployment     kubectl   scale   deployment   \u003C  deployment-nam  e  >   --replicas=  \u003C  num-replicas  >\n   Autoscale a deployment     kubectl   autoscale   deployment   \u003C  deployment-nam  e  >   --min=  \u003C  min-pods  >   --max=  \u003C  max-pods  >   --cpu-percent=  \u003C  target-CPU-utilization\n    \n## Rollouts and Versioning\n\nManage deployment rollouts.\n\n- **Check rollout status**\n```bash\nkubectl rollout status deployment/\u003Cdeployment-name>\n    View rollout history     kubectl   rollout   history   deployment/  \u003C  deployment-nam  e  >\n   Rollback to a previous revision     kubectl   rollout   undo   deployment/  \u003C  deployment-nam  e  >   --to-revision=  \u003C  revision  >\n  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",{"id":204,"path":205,"dir":178,"title":206,"description":207,"keywords":208,"body":211},"content:9.More:3.Kubeadm-Installation-Guide.md","/more/kubeadm-installation-guide","Kubernetes Cluster Setup Guide with Kubeadm","This guide provides instructions on how to set up a Kubernetes cluster using Kubeadm on a control plane and optionally add 1 or 2 nodes. Please follow these steps carefully to ensure a successful installation.",[209,210],"Prerequisites","Setting up the Control Plane","  Kubernetes Cluster Setup Guide with Kubeadm  This guide provides instructions on how to set up a Kubernetes cluster using Kubeadm on a control plane and optionally add 1 or 2 nodes. Please follow these steps carefully to ensure a successful installation.  Prerequisites  Before you begin, ensure you have the following:   Access to a virtual machine with admin permissions.  The system's firewall or restrictions should be disabled or configured to allow Kubernetes traffic.  Step 1: Prepare the Environment  Load Necessary Kernel Modules  Run the following commands to load the   overlay  and   br_netfilter  modules, which are required for Kubernetes networking:     cat   \u003C\u003C  EOF   |   sudo tee /etc/modules-load.d/containerd.conf\n   overlay\n   br_netfilter\n   EOF\n   \n   sudo   modprobe   overlay\n   sudo   modprobe   br_netfilter\n  Configure sysctl  Set up the required network settings:     cat   \u003C\u003C  EOF   |   sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\n   net.bridge.bridge-nf-call-iptables  = 1\n   net.ipv4.ip_forward                 = 1\n   net.bridge.bridge-nf-call-ip6tables = 1\n   EOF\n   \n   sudo   sysctl   --system\n  Step 2: Install Container Runtime  Update your package index and install   containerd  as the container runtime:     sudo   apt-get   update\n   sudo   apt-get   install   containerd   -y\n   sudo   mkdir   -p   /etc/containerd\n   sudo   containerd   config   default   |   sudo   tee   /etc/containerd/config.toml\n   sudo   systemctl   restart   containerd\n   sudo   systemctl   status   containerd\n  Ensure to disable swap:     sudo   swapoff   -a\n  Step 3: Add Kubernetes Repositories  Update your package index, install packages to allow apt to use a repository over HTTPS, and add Kubernetes' apt repository:     sudo   apt-get   update\n   sudo   apt-get   install   -y   apt-transport-https   ca-certificates   curl\n   \n   curl   -fsSL   https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key   |   sudo   gpg   --dearmor   -o   /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n   \n   echo   'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /'   |   sudo   tee   /etc/apt/sources.list.d/kubernetes.list\n   \n   sudo   apt   update\n  Step 4: Install Kubeadm, Kubelet, and Kubectl  Install the specific versions of Kubeadm, Kubelet, and Kubectl:     sudo   apt   install   -y   kubeadm=  1.28  .1-1.1   kubelet=  1.28  .1-1.1   kubectl=  1.28  .1-1.1\n   sudo   apt-mark   hold   kubelet   kubeadm   kubectl\n  Setting up the Control Plane  Initialize the control plane node using   kubeadm :     sudo   kubeadm   init   --pod-network-cidr=192.168.0.0/16   --kubernetes-version=1.28.0\n  Configure kubectl  Set up the kubeconfig file for access to your cluster:     mkdir   -p   $HOME  /.kube\n   sudo   cp   -i   /etc/kubernetes/admin.conf   $HOME  /.kube/config\n   sudo   chown   $(  id   -u  ):$(  id   -g  )   $HOME  /.kube/config\n  Apply a Pod Network  Install Calico for network policies and services:     kubectl   apply   -f   https://docs.projectcalico.org/manifests/calico.yaml\n  Joining Nodes to the Cluster  Generate a token to join your nodes to the cluster:     kubeadm   token   create   --print-join-command\n  Run the displayed   kubeadm join  command on each node you wish to add. Here's an example command:     sudo   kubeadm   join   10.0  .1.101:6443   --token   \u003C  toke  n  >   --discovery-token-ca-cert-hash   sha256:  \u003C  has  h  >\n  Label Nodes (Optional)  If you have worker nodes, label them for organization:     kubectl   label   nodes   \u003C  node-nam  e  >   node-role.kubernetes.io/worker=worker\n  Replace   \u003Cnode-name>  with the name of your node, such as   k8s-worker1  or   k8s-worker2 .  html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}",1709754083508]